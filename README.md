# **Table of Contents**
1. [Kubernetes](#kubernetes)<br>
  1.1. [Introduction](#introduction)<br>
  1.2. [Components](#components)<br>
  1.3. [Basic Requirements](#basic-requirements)<br>
  1.4. [Development Environment](#development-environment)<br>
  1.5. [Kubernetes in Practice](#kubernetes-in-practice)<br>
    1.5.1. [Development Environment](#development-environment)<br>
    1.5.2. [Article 01 - Persisting Data In Kubernetes](#article-01---persisting-data-in-kubernetes)<br>
    1.5.3. [Article 02 - How data "travels" between Pods](#article-02---how-data-travels-between-pods)<br>
    1.5.4. [Article 03 - Helm](#article-03---helm)<br>
    1.5.5. [Article 04 - Envoy](#article-04---envoy)<br>
2. [References](#references)<br>
  2.1. [Docker](#docker)<br>
  2.2. [Kubernetes](#kubernetes)<br>
  2.3. [Others](#others)<br>

# **Kubernetes (K8s)**

## **Introduction**
The biggest difference between Kubernetes and Docker is that the former is a **platform to run and manage containers from many different runtimes** while the latter is a **container runtime itself**[^1]. Meaning that we could use Kubernetes to manage a multitude of Docker containers (orchestration) we have created. Kubernetes will replicate or downsize according to a set of rules.

![Types of Deployment](https://d33wubrfki0l68.cloudfront.net/26a177ede4d7b032362289c6fccd448fc4a91174/eb693/images/docs/container_evolution.svg)

Kubernetes is an example of a Container Deployment. When we deploy a Kubernetes instance we have a Cluster that aims to manage a multitude of containers. Some key features[^2]:

* Recover instances as needed (Self-healing, high avaliability)
* Adapt to deeps and peaks of demand
* Offers environment consistency for development, testing, and production

## **Components**
Kubernetes is a little bit more complex than Docker, as expected since it orchestrates Docker containers, requiring more components to achieve that goal.

| **Node** | **Pod** | 
| ----------- | ----------- |
| An **instance of Kubernetes** is called Node, it is a physical, virtual[^3] or cloud machine. Each Node is managed by a control plane[^4]. | Inside Nodes we will find another component called **pod**, which is an **abstraction layer of a container**[^5]. This is where each application is running, each with an IP address generated by Kubernetes. If that app crashes for some reason, we would need to update the IP address manually in our code to make it communicate to each other again. Since that would be cumbersome, there are two other components: Service and Ingress. |

| **Service** | **Ingress** | 
| ----------- | ----------- |
| The **Service** is a static IP address that can be attached to each Pod[^5]. Service and Pod are not linked directly, they can exist separately and as soon as the Pod restarts the Service keeps its IP address static. | Now, **Ingress** is like a DNS server, solving all external requests into its correct Service. It opens the Node to the external world and proxy these requests to the correct Service. |

| **Configmap** | **Secret** | 
| ----------- | ----------- |
| **Configmap** is a component that lists external configuration of our application. With that we can set the database URL inside our app and no matter what the IP address is the database will reside on that particular URL so we don't need to update the application manually[^5]. | We could also add all passwords and users to Configmap. This approach is not safe and thus there's another similar component called **Secret** to achieve that goal. The file is encoded in base64 so it is safer than Configmap. |

### **Volumes**
Another important consequence of crashing containers is related to data. If we have a container with a database (Like MySQL) and it crashes, all data contained in it would be lost. This is why we have Volumes. **Volumes persists content to a physical drive (Local or Remote to the Node)**. K8s doesn't manage data persistance[^5].

| **Deployment** | **StatefulSet** | 
| ----------- | ----------- |
| Component responsible to create the abstraction layer of one or multiple applications (containers). Its content is not saved thus lost when a pod crashes, used to create State**LESS** apps. | Has the same function as Deployment but saves pod's internal data to create a State**FUL** app or Database |

## **Basic Requirements**

To develop this study you need to install Docker, Minikube, and Kubectl. To avoid explaining installation steps for each one of them, I decided to list the links for each application's documentation and you must make sure that they are running smoothly:

* Docker - https://docs.docker.com/engine/install/ubuntu/#set-up-the-repository
* Minikube - https://minikube.sigs.k8s.io/docs/start/
* Kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/

## **Development Environment**
I personally like to use [VS Code](https://code.visualstudio.com/) as my development software, for these reasons:

* Lightweight like Sublime but very capable due to extensions;
* Very customizable, every settings from every extension can be worked upon to match your needs;
* Supports Git protocol;
* Share your personal settings through GitHub, so all you need to do is load your user and your entire environment is ready;
* These beautifil extensions:
    * Docker: ms-azuretools.vscode-docker
    * Dev Containers: ms-vscode-remote.remote-containers

The first extension will make managing your images and containers (more below) through an UI, making everything easier if you prefer managing that way. 

The Dev Container extension enables us to "develop in production". By that I mean thatwhen you create your project using the extension, it will do all the heavy work of creating the required Docker files, start the container (app, contains a barebones OS within it) and then all VS Code "restarts" but inside the container running. There are advantages to this but I will keep it short and move on.

## **Kubernetes In Practice**

### **Article 01 - Persisting Data In Kubernetes**
In this article we will try to show how to persist data in a Kubernetes cluster. I will cover steps to create a MongoDB cluster with and without a Stateful component so that we can reset both instances and we will see what happens with the data we created. 

#### **Setup MongoDB deployment & service components**
First we need to setup the actual application by creating a Deployment and Service components. Using VSCode there's a feature that allows us to choose what component we are creating and then it creates the whole template file for you.

```yaml
# mongodb-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongodb
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          valueFrom:
            secretKeyRef:
              name: 
              key: 
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: 
              key: 
```

This is the basic deployment that would launch a MongoDB pod in our Cluster. 

The empty environmental variables (env section) will be filled in the next section. They are all related to basic authentication of the database and these are stored in a different component (secret).

The environmental variable names were taken from [MongoDB page in DockerHub](https://hub.docker.com/_/mongo) and are require in order that the deployment (pod) can access the service.

<br>

#### **Create MongoDB Secret component**
Create a new file to store the user and password for the DB access. Below you  may find the YAML file:

```yaml
# mongodb-secret.yaml
apiVersion: v1
kind: Secret
metadata:
    name: mongodb-secret
type: Opaque
data:
    mongo-root-username: Z3JhX2FkbWlu
    mongo-root-password: cGFzc3dvcmQ=
```

To get the root username and password there's an additional step. Remember that any actual information in a secret file must be base64 encoded. To encode in base64 we just need to run the following in the command line:

```$ echo -n <username or password> | base64```

Remember to put the value within '' if there's a space in it. This command will print out the value you requested in base64, paste those values in the corresponding key-value pair (mongo-root-username and mongo-root-password). also important to note that we could use any label for each data point within data section. Just make sure you use the same labels when pasting it back into the Deployment component.

After creating the file, make sure to add the component to your cluster ```$ kubectl apply -f mongo-secret.yaml``` and ```$ kubectl get secret``` to confirm that it was applied to the cluster.

![Secrets](https://user-images.githubusercontent.com/22838513/201800159-f4b93452-14c0-4ce0-a046-f78709d8a62f.png)

<br>

#### **Add Secret credentials do MongoDB Deployment and Create Service**
Now let's get back to MongoDB Deployment file (In my case mongodb-deployment.yaml) but keep the secret file opened. With the username and password generated in the step above, we can complete the Deployment component.

```yaml
# mongodb-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongodb
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          valueFrom:
            secretKeyRef:
              name: mongodb-secret # Secret YAML file name
              key: mongo-root-username # Name of the key in data section within Secret YAML file
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-password 
```

Now the Deployment component will access the Secret component directly, and if you want to change the credentials you only need to change the Secret component.

Next we will add the Service component directly in the Deployment component. This is good because we can reduce the number of files in our project since Deployment and Service are almost always created in pairs. 

To achieve that, add a "---" by the end of the current Deployment component and after that we start the Service component section:

```yaml
# Ending of mongodb-deployment.yaml file
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          valueFrom:
            secretKeyRef:
              name: mongodb-secret # Secret YAML file name
              key: mongo-root-username # Name of the key in data section within Secret YAML file
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-password 
---
apiVersion: v1
kind: Service
metadata:
  name: mongodb-service
spec:
  selector:
    app: mongodb
  ports:
    - protocol: TCP
      port: 27017
      targetPort: 27017 # This port must match above port (container/pod)
```

The ports are also somewhat standardized and can be found in the image's Docker Hub page or everywhere on the internet. With Secret and Service added to the Deployment file and kubcetl, we must now apply the deployment to the cluster:

```$ kubectl apply -f mongodb-deployment.yaml```

We can check the progress by typing ```$ kubectl get pod``` and ```$ kubectl get service```.

<br>

#### **Setup Mongo Express Deployment**
MongoDB is just the database engine that handles queries and data itself. Mongo Express is a container that will give MongoDB a UI, making it easier to manage database and data. This will also help us test the persistant state of the database (or not) in the next steps.

Let's create a file much like mongodb-deployment.yaml:

```yaml
# mongo-express-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo-express
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo-express
  template:
    metadata:
      labels:
        app: mongo-express
    spec:
      containers:
      - name: mongo-express
        image: mongo-express
        ports:
        - containerPort: 8081
        env:
        - name: ME_CONFIG_MONGODB_ADMINUSERNAME
          valueFrom:
            secretKeyRef:
              name:
              key: 
        - name: ME_CONFIG_MONGODB_ADMINPASSWORD
          valueFrom:
            secretKeyRef:
              name:
              key:
        - name: ME_CONFIG_MONGODB_SERVER
          valueFrom: 
            configMapKeyRef:
              name:
              key:
```

Just like what happened with MongoDB deployment, we need to gather three environmental variables from DockerHub. We need the admin username/password and the server where MongoDB is being hosted for Mongo Express to access it. To make it easier, we will also use the same secret as MongoDB.

The last environmental variable requires a ConfigMap, since we need to setup a URL for MongoDB pods.

<br>

#### **Create MongoDB ConfigMap**
Now we must create a file called ```mongodb-configmap.yaml`` and add the following information:

```yaml
# mongodb-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mongodb-configmap
data:
  database_url: mongodb-service
```

Since we are not deploying it online, the URL is going to be the service that is providing Mongo DB. This is why we have the service name as URL. Now we must apply this component to our cluster:

```$ kubectl apply -f mongodb-configmap.yaml```

<br>

#### **Add ConfigMap and Service to Mongo-Express Deployment**
With

```yaml
# Ending of mongo-express-deployment.yaml file
        ports:
        - containerPort: 8081
        env:
        - name: ME_CONFIG_MONGODB_ADMINUSERNAME
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-username
        - name: ME_CONFIG_MONGODB_ADMINPASSWORD
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-password
        - name: ME_CONFIG_MONGODB_SERVER
          valueFrom: 
            configMapKeyRef:
              name: mongodb-configmap
              key: database_url
---
apiVersion: v1
kind: Service
metadata:
  name: mongo-express-service
spec:
  selector:
    app: mongo-express
  type: LoadBalancer # This make the service externally available
  ports:
    - protocol: TCP
      port: 8081
      targetPort: 8081
      nodePort: 30000 # This is the external port that will be exposed
```
After completing the rest of the ```mongo-express-deployment.yaml```, we must apply that to our cluster: ```$ kubectl apply -f mongo-express-deployment.yaml```.

Now we have all necessary components to test what happens when we **don't** have the necessary components to persist data. Let's double check if all components are installed:

![kubectl get all](https://user-images.githubusercontent.com/22838513/201800204-f4dd4f81-53b1-40d7-a3a8-8f7075d9cef6.png)

In the above picture we may see - in order - **2 running pods (STATUS = RUNNING)**, **3 services (Kubernetes, mongo-express, and mongodb-deployment)**, **2 deployments**, and **2 replicaset**.

To verify the ports we could use ```$ kubectl get service```:

![Services](https://user-images.githubusercontent.com/22838513/201800224-28f74670-7f3a-4afa-bb2d-63ece7a0bfcc.png)

<br>

#### **Making Mongo Express Service available**
Minikube is a bit different than regular Kubernetes in this aspect. When you listed the Services, the Mongo Express Service won't have an external IP yet. To achieve that we have to run the following command:

```$ minikube service mongo-express-service```

Your Browser might open up after running this command, in my case it indicated that I was using Docker inside Linux and thus I should copy the address and paste it to my Browser:

![minikube service](https://user-images.githubusercontent.com/22838513/201800306-891ab5ca-e2a3-4896-b074-fddc8d2e7bf1.png)

![Mongo Express](https://user-images.githubusercontent.com/22838513/201800359-2397b916-f170-4580-a298-9988fab69c80.png)

<br>

#### **Testing data persistance**
With the service running, create a database with any name so we can test what happens when we reset the database pod.

![test-db](https://user-images.githubusercontent.com/22838513/201800379-e63bed74-335c-4511-bdc0-20e60bb5a4e6.png)


With the database created, let's restart the pod. There's no official way to do that within Kubernetes, we will scale the pods to 0 and back to 1 so we "reset" its state.

1) Reduce Replicas to 0: ```$ kubectl scale deployment mongodb-deployment --replicas=0```
2) Confirm that there's no instance of mongodb-deployment running: ```$ kubectl get deployment```
3) Increase Replicas back to 1: ```$ kubectl scale deployment mongodb-deployment --replicas=1```
4) Refresh your browser and check if the database we created (test-db) is gone.

</br>

### **Article 02 - How data "travels" between containers inside a pod**
This topic is a bit more complex and not that "useful" - lack of a better word. This will require monitoring the network for packages and is very specific to debug some solutions. We will utilize two images: Nginx and Busybox. Our deployment will have two containers. In article 01 we had one container per deployment.

```yaml
# nginx-bisybox.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-busybox
spec:
  containers:
  - command:
    - sleep
    - infinity
    image: busybox
    name: busybox
  - image: nginx
    name: nginx
```

?? I found articles talking about two kuberenetes nodes, not within one particular node (Above doesn't necessarily show packets, just processes)

https://www.redhat.com/sysadmin/kubernetes-pods-communicate-nodes

<br>

### **Article 03 - Helm**
Helm is a package manager for Kubernetes, much like the apt command in Linux. Instead of developing abunch of YAML files for all applications, other developers bundle them up into Helm Charts so that we can download it and use in our applications quicker. You can also think of it as Docker Hub or PyPi and their packages.

It can also work as a template engine, so that we will have templates and within these template YAML files we can change the placeholders "on the fly" which is a great feature when we have a Continuous Development scenario.

There're several ways to install Helm, make sure you [go to this link](https://helm.sh/docs/intro/install/#from-apt-debianubuntu) and verify the required steps to install Helm. For Ubuntu, I used the following commands:

```
$ curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
$ sudo apt-get install apt-transport-https --yes
$ echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
$ sudo apt-get update
$ sudo apt-get install helm
```

Run ``$ helm`` command to confirm that it was installed. If it was successful, we will now install a MongoDB using Helm.

We will build a Node.js application with MongoDB through Helm. This will also enable us to see persistance and replication of data. This article[^7] was used to develop this section, which was very helpful to understand how to verify if data was replicated throughout database instances.

1. [Clone Node Application](#clone-node-application)
2. [Preparing Node Application Docker Image](#preparing-node-application-docker-image)
3. [Implement MongoDB using Helm](#implement-mongodb-using-helm)
4. [Create Node.js App with personalized Helm chart](#create-nodejs-app-with-personalized-helm-chart)

<br>

#### **Clone Node Application**
The following command will clone the application repository to a folder called ``article_03``. This application contains a simple web app that gather two strings and store it in MongoDB. We will package it in a Docker image that will be used later on when scaling up our Node.js application.

```$ git clone https://github.com/do-community/node-mongo-docker-dev.git article_03```

There will be a lot of files in this project. We won't use every single one of them, I will slowly change them as needed. These changes are mostly related to newer versions becoming available and requiring us to adapt our code.

<br>

#### **Preparing Node Application Docker Image**

To make the connection between MongoDB pods, MongoDB uses a URI with all connection information. Currently, the file ``db.js`` stores that URI and we need to do minor changes to comply with newer versions. 

* Edit ``db.js``;
* Delete ``package-lock.json`` file;
* Edit ``Dockerfile``.

Add the following to ```db.js```:

```javascript
// db.js
...
const {
  MONGO_USERNAME,
  MONGO_PASSWORD,
  MONGO_HOSTNAME,
  MONGO_DB,
  MONGO_REPLICASET // Add this and remove MONGO_PORT
} = process.env;

...
const url = `mongodb://${MONGO_USERNAME}:${MONGO_PASSWORD}@${MONGO_HOSTNAME}/${MONGO_DB}?replicaSet=${MONGO_REPLICASET}&authSource=admin`; // Edit this connection string
...
```

We added ``MONGO_REPLICASET`` constant to the ``process.env`` variable and to the connection constant (``url``). With this we will pass the Replica address alongside the other parameters. We don't need ``MONGO_PORT`` anymore as the URI style changed.

Before we build a Docker image, delete ```package-lock.json``` and change the the first line of the Dockerfile:

```Dockerfile
FROM node:19.3.0 # Edit this image version

RUN mkdir -p /home/node/app/node_modules && chown -R node:node /home/node/app

WORKDIR /home/node/app

COPY package*.json ./

USER node

RUN npm install

COPY --chown=node:node . .

EXPOSE 8080

CMD [ "node", "app.js" ]
```

Theses changes were made to ensure that we are using a current version of Node.js. Now we build the image locally and push it to our DockerHub repository:

```$ docker build -t your_dockerhub_username/article-03-node-replicas .```

After build ran successful, push this image to your DockerHub repo, in my case:

```$ docker push vitorcmonteiro/article-03-node-replicas```

![Docker push to Docker Hub](https://user-images.githubusercontent.com/22838513/208545954-33dbac92-e308-49ae-a181-3d96f6268570.png)


#### **Implement MongoDB using Helm**
For this step we will take advantage of Helm's templating. We must create a file called ``mongodb-values.yaml`` in the root folder and that file will serve as the values that will be inserted into the Helm Chart template when it creates the Deployments.

To acomplish this we will use [Bitnami's image](https://artifacthub.io/packages/helm/bitnami/mongodb) as it is still being updated and is fairly famous. We are going to create a cluster of the [ReplicaSet (StatefulSet)](https://www.mongodb.com/docs/kubernetes-operator/master/tutorial/mdb-resources-arch/#replica-set) type. The pods interact with a headless service (No particular deployment associated) and that will require us to create a secret that will be shared by any additional pod we create.

First, we create a key for the ReplicaSet:

```$ openssl rand -base64 756 > key.txt``` and add that to the cluster like we did with the secret ```$ kubectl create secret generic keyfilesecret --from-file=key.txt```. Since we already created MongoDB's secret for admin access, we will have two secrets.

After that we can create the ``mongodb-values.yaml`` file. This file is needed in order to configure the Helm package we are installing and acts as a template.

```yaml
# mongdb-values.yaml
architecture: replicaset
replicaCount: 3
persistence:
  size: 1Gi
auth:
  rootUser: gra_admin
  rootPassword: "password"
  existingKeySecret: keyfilesecret
```
By default, this MongoDB chart uses a standalone approach (1 pod, centralized). Thus we added the architecture key and set it to replicaset. Then we set the number of pods (scale of our database), the size of each volume, and the authentication information.

Now we will install MongoDB using Helm (instead of creating all files like we did in article 01):

```$ helm repo add mongodb https://charts.bitnami.com/bitnami```<br>
```$ helm install mongodb -f mongodb-values.yaml mongodb/mongodb```

![Helm installation](https://user-images.githubusercontent.com/22838513/208546022-55175695-69f6-4e01-b4c4-d7b72f4c735a.png)

Just by running these commands we will have a running pod named helm-mongodb, check that by running ``$ kubectl get pod``:

![Pods running](https://user-images.githubusercontent.com/22838513/208546072-2ab23b28-1883-4853-b009-fe3d27d88d56.png)

Now that the pods are running, we have a MongoDB installed and running in multiple instances. Next up we are going to create our own Helm package instead of using a Bitnami's or someone elese's repository.

TODO: Quick explanation of linking between PV and PVCs

<br>

#### **Create Node.js App with personalized Helm chart**
At the root level of the project, create a new Helm repository:

```$ helm create nodeapp```

This command will create a folder called ``nodeapp`` and with a bunch of files in it. To explain a few important ones.

* Chart.yaml - Basic information about the Chart.
* values.yaml - Set of specific parameters.
* templates/ - Contains all template files that will generate Kubernetes components.
* charts/ - Contains any charts that this chart depends on.

First we will edit values.yaml to match the image we created at the first step.

```yaml
# values.yaml
# Default values for nodeapp.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 3

image:
  repository: vitorcmonteiro/article-03-node-replicas
  tag: latest
  pullPolicy: Always

nameOverride: ""
fullnameOverride: ""

service:
  type: LoadBalancer # Here
  port: 80
  targetPort: 8080 # Add this line
...
```

Why are we doing this? Well, remember that the first step consisted in creating an image that would pull Node.js image and run it as a web-server (Take a look at the Dockerfile). So every time we want to increase the replica amount, it will run that image and launch a pod (container) in minikube.

Now create a file called ``secret.yaml`` in the ``templates`` folder and insert the following:

```yaml
# secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: {{ .Release.Name }}-auth
data:
  MONGO_USERNAME: Z3JhX2FkbWlu
  MONGO_PASSWORD: cGFzc3dvcmQ
```

MONGO_USERNAME and MONGO_PASSWORD came from the very first secret file we created (article 01) and it's base64-encoded. This secret will replicate for each Node pod that Kubernet launches, guaranteeing that all instances have access to the database using these credentials. Remember that there's another component called ConfigMap where we stored all variables for each deployment.

Create a ``configmap.yaml`` file inside the ``templates`` folder and add the following:

```yaml
# configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-config
data:
  MONGO_HOSTNAME: "mongodb-0.mongodb-headless.default.svc.cluster.local:27017,mongodb-1.mongodb-headless.default.svc.cluster.local:27017,mongodb-2.mongodb-headless.default.svc.cluster.local:27017"  
  MONGO_PORT: "27017"
  MONGO_DB: "sharkinfo"
  MONGO_REPLICASET: "rs0"
```

Hostname depends on the name of your pods. That means it's a good idea to first ```$ kubectl get pods``` and see mongodb pods names and then add them in Configmap. They are usually named like this: ```$(statefulset-name)-$(ordinal).$(service name).$(namespace).svc.cluster.local```. Since we have a headless service that will proxy queries to each pod, you see mongodb-headless in all addresses.

I think this solution is very limited because what happens if we add another instance of MongoDB? I couldn't figure out and didn't want to spend too much time on it but I believe it is possible.

Now we have to work on ```deployment.yaml``` file which will iteratively run deploying our web application as many times as we need. Right below the key named ```imagePullPolicy```, let's create a ```env``` key:

```yaml
# deployment.yaml
          env:
          - name: MONGO_USERNAME
            valueFrom:
              secretKeyRef:
                key: MONGO_USERNAME
                name: {{ .Release.Name }}-auth
          - name: MONGO_PASSWORD
            valueFrom:
              secretKeyRef:
                key: MONGO_PASSWORD
                name: {{ .Release.Name }}-auth
          - name: MONGO_HOSTNAME
            valueFrom:
              configMapKeyRef:
                key: MONGO_HOSTNAME
                name: {{ .Release.Name }}-config
          - name: MONGO_PORT
            valueFrom:
              configMapKeyRef:
                key: MONGO_PORT
                name: {{ .Release.Name }}-config
          - name: MONGO_DB
            valueFrom:
              configMapKeyRef:
                key: MONGO_DB
                name: {{ .Release.Name }}-config      
          - name: MONGO_REPLICASET
            valueFrom:
              configMapKeyRef:
                key: MONGO_REPLICASET
                name: {{ .Release.Name }}-config
...
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /sharks # Here
              port: http
          readinessProbe:
            httpGet:
              path: /sharks # Here
              port: http
...
```

These variables will be injected into each Node.js pod and will be used by ```db.js``` to connect to MongoDB service who will proxy the query to one of MongoDB's pods. Now we can install/deploy it:

```$ helm install nodejs ./nodeapp```

#### **Open port to local machine and test**
After confirming that all pods are running, open Node.js service port and access it in the browser:

```$ minikube service nodejs-nodeapp```

Add some sharks and see that the results add up when you add a new one. Even if we stop minikube entirely we would persist our data accross all pods.
<br>

### **Article 04 - Envoy**
Envoy is a proxy/load balancing technology created at Lyft. There are several other options like NGINX, HAProxy, Zuul, Linkerd, Traefik, and Caddy (Go) [^8]. It's important to note that depending on the engine used in the cloud service this tutorial may not work.

* Enable underlying pods to be accessed externally;
* Used as a networking map, connecting the dots between ingress and services;
* Treat incoming requests and adapts it to match underlying API's requirements, integrating them into one single app;

Remember that pods themselves are not open to the outside world. That's one of the strengths of Kubernetes (and Minikube). To connect from the outside we must use one of the two types of objects: [LoadBalancer](https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/) or [Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/).

These steps are based on [this article](https://www.getambassador.io/resources/ambassador-prometheus) and it really helped me setup my Prometheus-Ambassador solution.

#### **Install required CRDs (Custom Resource Definitions) & Deploy Prometheus Operator**
First, we need to create something called [Prometheus Operator](https://www.tigera.io/learn/guides/prometheus-monitoring/prometheus-operator/). This operator will manage the Prometheus application and automate all configurations. RBAC is just the short for Role-based access control which is basically creating accounts with certain access levels (Very much what happens with Cloud Services).

* Apply CRDs
``$ kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/bundle.yaml
``

* Check if they were created
* Check if Prometheus is running

#### **Install RBACs to allow monitoring**
Now, we need to create accounts that will take the role of monitoring our application:

```yaml
# prom-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["get"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: default
```

After creating the above file, apply it to our cluster:</br>
``$ kubectl apply -f prom-rbac.yaml``

#### **Deploy Prometheus instance**
To create a Prometheus deployment we will use the following:

```yaml
# prometheus.yaml
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus
spec:
  serviceAccountName: prometheus
  serviceMonitorSelector:
    matchLabels:
      ambassador: monitoring
  resources:
    requests:
      memory: 400Mi
```

Then apply it to the cluster as well:</br>
```$ kubectl apply -f prometheus.yaml```

Make sure it's running and then port forward the app so we can reach through the browser:</br>
```$ kubectl port-forward svc/prometheus-operated 9090:9090```

[prometheus-ui]

#### **Configure ServiceMonitor**
These CRDs are used to set targets to be monitored by Prometheus. Let's create one:

```yaml
# service-monitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: prometheus
  labels:
    name: prometheus
spec:
  selector:
    matchLabels:
      operated-prometheus: "true"
  namespaceSelector:
    any: true
  endpoints:
    - port: web
```
This ServiceMonitor will look for namespaces with operated-prometheus in it and will scrap all important data and feed it into Prometheus.

### TODO: How to stress test (POST requests? Limit resources to a minimum)</br>

https://github.com/markvincze/PrimeCalcApi/

### **Developing inside a container with VS Code**
References:</br>
[VS Code Documentation](https://code.visualstudio.com/docs/devcontainers/containers)</br>
[Dev Containers Tutorial](https://code.visualstudio.com/docs/devcontainers/tutorial)

To develop inside a running container you need the following:
* Docker CE/EE edition running on your Linux Distro;
* Install Visual Studio Code;
* Install [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers);
* Run VS Code click the following icon:

![icon-image]()

* Select "Create Dev Container..." and then you can select from a list of pre-built images.

# References
## Docker
[^1]: [Docker Compose page](https://docs.docker.com/compose/) <br>
[^2]: [Docker Compose page about Services tag](https://docs.docker.com/compose/compose-file/#services-top-level-element)<br>
https://docs.docker.com/get-started/<br>
https://docker-curriculum.com/<br>
https://www.youtube.com/watch?v=rmf04ylI2K0<br>
https://medium.com/@lawrence-han/an-introduction-to-containers-docker-and-kubernetes-eb79449003c<br>
https://docs.docker.com/samples/<br>

## Kubernetes
[^1]: [Kubernetes Overview](https://kubernetes.io/docs/concepts/overview/)<br>
[^2]: [Kubernetes Features](https://www.guru99.com/kubernetes-tutorial.html)<br>
[^3]: [Kubernetes Nodes](https://kubernetes.io/docs/concepts/architecture/nodes/)<br>
[^4]: [Kubernetes Glossary](https://kubernetes.io/docs/reference/glossary/)<br>
[^5]: [YouTube Kubernetes Tutorial for Beginners (4 hours)](https://www.youtube.com/watch?v=X48VuDVv0do)<br>
[^6]: [How To Deploy MongoDB on Kubernetes â€“ Beginners Guide](https://devopscube.com/deploy-mongodb-kubernetes/)<br>
[^7]: [How To Scale a Node.js Application with MongoDB on Kubernetes Using Helm](https://www.digitalocean.com/community/tutorials/how-to-scale-a-node-js-application-with-mongodb-on-kubernetes-using-helm)<br>
[^8]: [How to use Envoy as a Load Balancer in Kubernetes](https://blog.markvincze.com/how-to-use-envoy-as-a-load-balancer-in-kubernetes/)<br>
[^9]: [Set up Ingress on Minikube with the NGINX Ingress Controller](https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/)<br>
[^9]: [Envoy 1.24.1 Documentation](https://www.envoyproxy.io/docs/envoy/v1.24.1/)<br>
[^10]: [Installing Grafana in Kubernetes](https://grafana.com/docs/grafana/latest/setup-grafana/installation/kubernetes/)<br>

https://docs.docker.com/desktop/kubernetes/<br>
https://www.atlassian.com/microservices/microservices-architecture/kubernetes-vs-docker<br>
https://www.freecodecamp.org/news/kubernetes-vs-docker-whats-the-difference-explained-with-examples/<br>
https://www.freecodecamp.org/news/the-kubernetes-handbook/<br>
https://speedscale.com/how-to-test-kubernetes-autoscaling/<br>
https://www.redhat.com/sysadmin/kubernetes-pod-network-communications<br>
https://phoenixnap.com/kb/kubernetes-mongodb<br>
https://loft.sh/blog/kubernetes-statefulset-examples-and-best-practices/<br>
https://devopscube.com/setup-prometheus-monitoring-on-kubernetes/#:~:text=Prometheus%20is%20a%20high%2Dscalable,helps%20with%20metrics%20and%20alerts.<br>
https://docker-curriculum.com/<br>
https://docs.docker.com/desktop/install/ubuntu/<br>
https://www.youtube.com/watch?v=E-UpGmj6B9M<br>
https://www.sobyte.net/post/2021-11/envoy-usage-demo/<br>

## Others

https://github.com/bitnami/charts/tree/main/bitnami/mongodb<br>
https://www.getambassador.io/resources/ambassador-prometheus<br>
https://blog.opstree.com/2022/10/04/prometheus-and-grafana-on-kubernetes/<br>
https://blog.markvincze.com/how-to-use-envoy-as-a-load-balancer-in-kubernetes/<br>
https://www.redhat.com/sysadmin/kubernetes-pod-network-communications<br>
https://www.redhat.com/sysadmin/kubernetes-pods-communicate-nodes<br>

https://observability.thomasriley.co.uk/prometheus/deploying-prometheus/launch-prometheus-instance/
https://docs.openshift.com/container-platform/3.11/rest_api/rbac_authorization_k8s_io/clusterrole-rbac-authorization-k8s-io-v1.html#clusterrole-rbac-authorization-k8s-io-v1